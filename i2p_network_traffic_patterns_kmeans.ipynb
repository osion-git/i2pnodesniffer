{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# I2P k-Means-Algorithmus\n",
    "Es wird ein k-Means-Algorithmus angewendet."
   ],
   "id": "49e686907b4887e3"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from collections import Counter\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scapy.all import rdpcap, IP, TCP, UDP, Raw\n",
    "from scipy.stats import entropy\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler"
   ],
   "id": "ded4a4d05918310a",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "source_ip = '10.8.0.2'\n",
    "target_ip = '10.8.0.11'\n",
    "packets = rdpcap('traffic_with_filter.pcap')"
   ],
   "id": "acf390d4e4af4bfe",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "filter_with_source_ip = True\n",
    "\n",
    "def filter_packets(packets):\n",
    "    return [\n",
    "        (frame_number, pkt) for frame_number, pkt in enumerate(packets, start=1)\n",
    "        if IP in pkt\n",
    "           and pkt[IP].dst == target_ip\n",
    "           and (pkt[IP].src == source_ip if filter_with_source_ip else True)\n",
    "    ]\n",
    "filtered_packets = filter_packets(packets)\n",
    "print(\n",
    "    f\"Number of packets to {target_ip}{' and from ' + source_ip if 'source_ip' in globals() and source_ip else ''}: {len(filtered_packets)}\")"
   ],
   "id": "1b097e85eb307f01",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Features\n",
    "Werden gelesen und für k-Means vorbereitet."
   ],
   "id": "d62dab42413bb10f"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def byte_histogram(payload: bytes):\n",
    "    counts = Counter(payload)\n",
    "    hist = np.array([counts.get(i, 0) for i in range(256)])\n",
    "    return hist / hist.sum() if hist.sum() > 0 else np.zeros(256)\n",
    "\n",
    "def calc_entropy(payload: bytes):\n",
    "    if not payload:\n",
    "        return 0.0\n",
    "    counts = Counter(payload)\n",
    "    probs = np.array(list(counts.values())) / len(payload)\n",
    "    return entropy(probs, base=2)\n",
    "features = []\n",
    "frame_numbers = []\n",
    "\n",
    "def build_with_protocol():\n",
    "    for frame_number, pkt in filtered_packets:\n",
    "        if Raw in pkt:\n",
    "            payload = bytes(pkt[Raw].load)\n",
    "        if TCP in pkt:\n",
    "            protocol = \"tcp\"\n",
    "            src_port = pkt[TCP].sport\n",
    "            dst_port = pkt[TCP].dport\n",
    "        elif UDP in pkt:\n",
    "            protocol = \"udp\"\n",
    "            src_port = pkt[UDP].sport\n",
    "            dst_port = pkt[UDP].dport\n",
    "        else:\n",
    "            protocol = \"unknown\"\n",
    "            src_port = 0\n",
    "            dst_port = 0\n",
    "        payload_len = len(payload)\n",
    "        hist = byte_histogram(payload)\n",
    "        protocol_encoded = 0 if protocol == \"tcp\" else (1 if protocol == \"udp\" else 2)\n",
    "        vec = [src_port, dst_port, payload_len, protocol_encoded, *hist]\n",
    "        frame_numbers.append(frame_number)\n",
    "        features.append(vec)"
   ],
   "id": "2cd149a06a643897",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def create_feature_columns():\n",
    "    df = pd.DataFrame(features)\n",
    "    df.columns = ['src_port', 'dst_port', 'payload_len', 'protocol'] + [f'byte_{i}' for i in range(256)]\n",
    "    df.head()\n",
    "    scaler = StandardScaler()\n",
    "    return scaler.fit_transform(df)"
   ],
   "id": "b81445bd8075283f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "build_with_protocol()\n",
    "X_scaled = create_feature_columns()"
   ],
   "id": "32ccd451a99ab028",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Elbow-Auswertung",
   "id": "7b14941d50a10240"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "inertia = []\n",
    "k_range = range(1, 55)\n",
    "\n",
    "for k in k_range:\n",
    "    km = KMeans(n_clusters=k, random_state=42)\n",
    "    km.fit(X_scaled)\n",
    "    inertia.append(km.inertia_)\n",
    "\n",
    "plt.plot(k_range, inertia, marker='o')\n",
    "plt.title(\"\")\n",
    "plt.xlabel(\"Anzahl Cluster (k)\")\n",
    "plt.ylabel(\"Verzehrung\")\n",
    "plt.grid(True)\n",
    "plt.xticks(range(1, 55, 3))\n",
    "plt.show()"
   ],
   "id": "6f3ec02352d5f8f4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Anzahl Punkte pro Cluster zählen",
   "id": "9bfc88e9dfab25d5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "optimal_k = 49\n",
    "km_optimal = KMeans(n_clusters=optimal_k, random_state=42)\n",
    "cluster_labels = km_optimal.fit_predict(X_scaled)\n",
    "\n",
    "# Collect clusters\n",
    "cluster_dict = {}\n",
    "for index, label in enumerate(cluster_labels):\n",
    "    cluster_dict.setdefault(label, []).append(index)\n",
    "\n",
    "# Sort by size\n",
    "sorted_clusters = sorted(cluster_dict.items(), key=lambda item: len(item[1]), reverse=True)\n",
    "\n",
    "# Output: Cluster with frame numbers\n",
    "for cluster, indices in sorted_clusters:\n",
    "    frame_ids = [frame_numbers[i] for i in indices]\n",
    "    print(f\"Cluster {cluster} ({len(indices)} points): Frames {frame_ids}\\n\")\n",
    "\n",
    "# Average\n",
    "total_points = sum(len(indices) for _, indices in sorted_clusters)\n",
    "average_points = total_points / optimal_k\n",
    "print(f\"⟶ Average number of points per cluster: {average_points:.2f}\\n\")\n",
    "\n",
    "# Summarize cluster sizes\n",
    "cluster_size_counts = {}\n",
    "for indices in cluster_dict.values():\n",
    "    size = len(indices)\n",
    "    cluster_size_counts[size] = cluster_size_counts.get(size, 0) + 1\n",
    "\n",
    "print(\"Distribution of cluster sizes:\")\n",
    "for size in sorted(cluster_size_counts):\n",
    "    count = cluster_size_counts[size]\n",
    "    print(f\"{size} data points: {count} clusters\")\n"
   ],
   "id": "508417798fb14a06",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "8c3920c9949413b6",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.x"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
